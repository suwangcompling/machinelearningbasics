{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Softmax Classification\n",
    "\n",
    "* Probability that $y$ is of class $i$: $ P(y = i|x,w,b) = softmax_x(w\\cdot x + b) = \\frac{e^{w_ix+b_i}}{\\sum_je^{w_jx+b_j}}  $\n",
    "* Prediction: $ yHat = argmax P(y = i|x,w,b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Loss Function\n",
    "\n",
    "* $ L(\\theta=\\{W,b\\},D) = -L(\\theta=\\{W,b\\},D) = -\\sum_{i=0}^{|D|} log P(Y = y_i | x_i, W,b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import six.moves.cPickle as pickle\n",
    "import gzip, os, sys, timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import function, shared\n",
    "from theano.tensor.nnet import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, inpt, nIn, nOut):\n",
    "        # k (1..K): class index; i (1..N): data point index; d: number of predictors.\n",
    "        # W (d x K matrix): column k = weights for class k.\n",
    "        # x (1 x d vector): in dataset X, row i = data point i.\n",
    "        # b (1 x K vector): element k = bias for class k.\n",
    "        self.W = shared(value=np.zeros((nIn,nOut),dtype=theano.config.floatX), \n",
    "                        name='W', borrow=True) # W is a matrix.\n",
    "        self.b = shared(value=np.zeros((nOut,),dtype=theano.config.floatX), \n",
    "                        name='b', borrow=True) # b is a vector.\n",
    "        self.pYgivenX = softmax(T.dot(inpt, self.W) + self.b)\n",
    "        self.yHat = T.argmax(self.pYgivenX, axis=1)\n",
    "        self.params = [self.W, self.b]\n",
    "        self.inpt = inpt\n",
    "        \n",
    "    def nll(self, y): # negative log-likelihood.\n",
    "        # type(y): T.TensorType\n",
    "        return -T.mean(T.log(self.pYgivenX)[T.arange(y.shape[0]), y])\n",
    "    \n",
    "    def errors(self, y):\n",
    "        # returns a float, the number of errors in the minibatch.\n",
    "        # type(y): T.TensorType\n",
    "        assert y.ndim == self.yHat.ndim\n",
    "        assert y.dtype.startswith('int')\n",
    "        return T.mean(T.neq(self.yHat, y)) # .neq returns a vector where 1 for a misprediction.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shared_dataset(data, borrow=True):\n",
    "    X, Y = data\n",
    "    sharedX = theano.shared(np.asarray(X,dtype=theano.config.floatX), borrow=borrow)\n",
    "    sharedY = theano.shared(np.asarray(Y,dtype=theano.config.floatX), borrow=borrow)\n",
    "    return sharedX, T.cast(sharedY, 'int32') \n",
    "def load_mnist():\n",
    "    import os\n",
    "    os.chdir(\"/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/ML_GENERAL/PYTHON_IMPL/DATA/\")\n",
    "    with gzip.open('mnist.pkl.gz') as f:\n",
    "        data_train, data_dev, data_test = pickle.load(f)\n",
    "    X_train, Y_train = shared_dataset(data_train)\n",
    "    X_dev, Y_dev = shared_dataset(data_dev)\n",
    "    X_test, Y_test = shared_dataset(data_test)\n",
    "    \n",
    "    return [(X_train, Y_train), (X_dev, Y_dev), (X_test, Y_test)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd(lr=.1, epochs=1000, batchSize=600, data=load_mnist): # dataset='mnist.pkl.gz' as 3rd arg\n",
    "    \n",
    "    datasets = load_mnist()\n",
    "    X_train, Y_train = datasets[0]\n",
    "    X_dev, Y_dev = datasets[1]\n",
    "    X_test, Y_test = datasets[2]\n",
    "\n",
    "    nTrainBatches = X_train.get_value(borrow=True).shape[0] / batchSize # // if Python 3.\n",
    "    nDevBatches = X_dev.get_value(borrow=True).shape[0] / batchSize\n",
    "    nTestBatches = X_test.get_value(borrow=True).shape[0] / batchSize\n",
    "    print \"... building the model\"\n",
    "    index = T.iscalar() # index of a batch.\n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y')\n",
    "    classifier = LogisticRegression(inpt=x, nIn=28*28, nOut=10)\n",
    "    cost = classifier.nll(y)\n",
    "    test_model = function(inputs=[index], outputs=classifier.errors(y),\n",
    "                          givens = {x: X_test[index*batchSize: (index+1)*batchSize],\n",
    "                                    y: Y_test[index*batchSize: (index+1)*batchSize]})\n",
    "    dev_model = function(inputs=[index], outputs=classifier.errors(y),\n",
    "                         givens = {x: X_dev[index*batchSize: (index+1)*batchSize],\n",
    "                                   y: Y_dev[index*batchSize: (index+1)*batchSize]})\n",
    "        # if not do 'givens' trick, the batch size has to be fixed in building symbolic graph.\n",
    "        # 'givens' trick modify the graph to compile before compiling it. \n",
    "        # in other words, we substitute in the graph, the key in givens with the associated value. \n",
    "    gW = T.grad(cost=cost, wrt=classifier.W)\n",
    "    gb = T.grad(cost=cost, wrt=classifier.b)\n",
    "    updates = [(classifier.W, classifier.W - lr*gW), (classifier.b, classifier.b - lr*gb)]\n",
    "    train_model = function(inputs=[index], outputs=cost, updates=updates, \n",
    "                           givens = {x: X_train[index*batchSize: (index+1)*batchSize],\n",
    "                                     y: Y_train[index*batchSize: (index+1)*batchSize]})\n",
    "    print \"... training the model\"\n",
    "    \n",
    "    patience = 5000 # for early stop, but examine this many data points regardless.\n",
    "    patienceIncrease = 2\n",
    "    improvementThreshold = .995\n",
    "    validationFrequency = min(nTrainBatches, patience/2) # examine this many batches before validation check.\n",
    "    bestValidationLoss = np.inf\n",
    "    testScore = .0\n",
    "    startTime = timeit.default_timer()\n",
    "    doneLooping = False\n",
    "    epoch = 0\n",
    "    while (epoch < epochs) and (not doneLooping):\n",
    "        epoch += 1\n",
    "        for batchIndex in range(nTrainBatches):\n",
    "            avgBatchCost = train_model(batchIndex)\n",
    "            iter = (epoch-1)*nTrainBatches + batchIndex\n",
    "            if (iter+1) % validationFrequency == 0:\n",
    "                validationLoss = [dev_model(i) for i in range(nDevBatches)]\n",
    "                thisValidationLoss = np.mean(validationLoss)\n",
    "                print \"Epoch %i, Batch %i/%i, Validation Error %f %%\" % (epoch, batchIndex+1,\n",
    "                                                                         nTrainBatches, thisValidationLoss*100)\n",
    "                if thisValidationLoss < bestValidationLoss:\n",
    "                    if thisValidationLoss < bestValidationLoss*improvementThreshold:\n",
    "                        patience = max(patience, iter*patienceIncrease)\n",
    "                    bestValidationLoss = thisValidationLoss\n",
    "                    testLosses = [test_model(i) for i in range(nTestBatches)]\n",
    "                    testScore = np.mean(testLosses)\n",
    "                    print \"Epoch %i, Batch %i/%i, Test Error of Best %f %%\" % (epoch, batchIndex+1,\n",
    "                                                                               nTrainBatches, testScore*100)\n",
    "                    with open('best_model.pkl','wb') as f:\n",
    "                        pickle.dump(classifier, f)\n",
    "                if patience <= iter:\n",
    "                    doneLooping = True\n",
    "                    break\n",
    "    endTime = timeit.default_timer()\n",
    "    print \"Optimization complete with best validation score of %f %%, best test performance %f %% \" % (bestValidationLoss*100, testScore*100)\n",
    "    print \"The code run for %d epochs, with %f epochs/sec\" % (epoch, 1.*epoch/(endTime-startTime))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_mnist(evaluateSize=1000, test=True):\n",
    "    classifier = pickle.load(open('best_model.pkl'))\n",
    "    predict_model = function(inputs=[classifier.inpt], outputs=classifier.yHat)\n",
    "    datasets = load_mnist()\n",
    "    if test:\n",
    "        X_test, Y_test = datasets[2]\n",
    "        X_test = X_test.get_value()\n",
    "        correct = sum(predict_model(X_test[:evaluateSize]) == Y_test[:evaluateSize].eval())\n",
    "    else:\n",
    "        X_train, Y_train = datasets[0]\n",
    "        X_train = X_train.get_value()\n",
    "        correct = sum(predict_model(X_train[:evaluateSize]) == Y_train[:evaluateSize].eval())\n",
    "    print \"Accuracy: {}/{} ({}%)\".format(correct,evaluateSize,float(correct)/evaluateSize*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model\n",
      "... training the model\n",
      "Epoch 1, Batch 83/83, Validation Error 13.020833 %\n",
      "Epoch 1, Batch 83/83, Test Error of Best 13.052083 %\n",
      "Epoch 2, Batch 83/83, Validation Error 11.572917 %\n",
      "Epoch 2, Batch 83/83, Test Error of Best 11.520833 %\n",
      "Epoch 3, Batch 83/83, Validation Error 10.697917 %\n",
      "Epoch 3, Batch 83/83, Test Error of Best 10.635417 %\n",
      "Epoch 4, Batch 83/83, Validation Error 10.239583 %\n",
      "Epoch 4, Batch 83/83, Test Error of Best 10.229167 %\n",
      "Epoch 5, Batch 83/83, Validation Error 9.895833 %\n",
      "Epoch 5, Batch 83/83, Test Error of Best 9.906250 %\n",
      "Epoch 6, Batch 83/83, Validation Error 9.572917 %\n",
      "Epoch 6, Batch 83/83, Test Error of Best 9.635417 %\n",
      "Epoch 7, Batch 83/83, Validation Error 9.416667 %\n",
      "Epoch 7, Batch 83/83, Test Error of Best 9.364583 %\n",
      "Epoch 8, Batch 83/83, Validation Error 9.281250 %\n",
      "Epoch 8, Batch 83/83, Test Error of Best 9.208333 %\n",
      "Epoch 9, Batch 83/83, Validation Error 9.197917 %\n",
      "Epoch 9, Batch 83/83, Test Error of Best 9.020833 %\n",
      "Epoch 10, Batch 83/83, Validation Error 9.052083 %\n",
      "Epoch 10, Batch 83/83, Test Error of Best 8.979167 %\n",
      "Epoch 11, Batch 83/83, Validation Error 8.937500 %\n",
      "Epoch 11, Batch 83/83, Test Error of Best 8.854167 %\n",
      "Epoch 12, Batch 83/83, Validation Error 8.854167 %\n",
      "Epoch 12, Batch 83/83, Test Error of Best 8.802083 %\n",
      "Epoch 13, Batch 83/83, Validation Error 8.770833 %\n",
      "Epoch 13, Batch 83/83, Test Error of Best 8.656250 %\n",
      "Epoch 14, Batch 83/83, Validation Error 8.666667 %\n",
      "Epoch 14, Batch 83/83, Test Error of Best 8.520833 %\n",
      "Epoch 15, Batch 83/83, Validation Error 8.625000 %\n",
      "Epoch 15, Batch 83/83, Test Error of Best 8.447917 %\n",
      "Epoch 16, Batch 83/83, Validation Error 8.552083 %\n",
      "Epoch 16, Batch 83/83, Test Error of Best 8.395833 %\n",
      "Epoch 17, Batch 83/83, Validation Error 8.520833 %\n",
      "Epoch 17, Batch 83/83, Test Error of Best 8.333333 %\n",
      "Epoch 18, Batch 83/83, Validation Error 8.458333 %\n",
      "Epoch 18, Batch 83/83, Test Error of Best 8.322917 %\n",
      "Epoch 19, Batch 83/83, Validation Error 8.416667 %\n",
      "Epoch 19, Batch 83/83, Test Error of Best 8.291667 %\n",
      "Epoch 20, Batch 83/83, Validation Error 8.354167 %\n",
      "Epoch 20, Batch 83/83, Test Error of Best 8.260417 %\n",
      "Epoch 21, Batch 83/83, Validation Error 8.312500 %\n",
      "Epoch 21, Batch 83/83, Test Error of Best 8.218750 %\n",
      "Epoch 22, Batch 83/83, Validation Error 8.270833 %\n",
      "Epoch 22, Batch 83/83, Test Error of Best 8.125000 %\n",
      "Epoch 23, Batch 83/83, Validation Error 8.260417 %\n",
      "Epoch 23, Batch 83/83, Test Error of Best 8.072917 %\n",
      "Epoch 24, Batch 83/83, Validation Error 8.239583 %\n",
      "Epoch 24, Batch 83/83, Test Error of Best 8.041667 %\n",
      "Epoch 25, Batch 83/83, Validation Error 8.218750 %\n",
      "Epoch 25, Batch 83/83, Test Error of Best 8.000000 %\n",
      "Epoch 26, Batch 83/83, Validation Error 8.218750 %\n",
      "Epoch 27, Batch 83/83, Validation Error 8.229167 %\n",
      "Epoch 28, Batch 83/83, Validation Error 8.197917 %\n",
      "Epoch 28, Batch 83/83, Test Error of Best 7.958333 %\n",
      "Epoch 29, Batch 83/83, Validation Error 8.166667 %\n",
      "Epoch 29, Batch 83/83, Test Error of Best 7.916667 %\n",
      "Epoch 30, Batch 83/83, Validation Error 8.156250 %\n",
      "Epoch 30, Batch 83/83, Test Error of Best 7.947917 %\n",
      "Epoch 31, Batch 83/83, Validation Error 8.135417 %\n",
      "Epoch 31, Batch 83/83, Test Error of Best 7.937500 %\n",
      "Epoch 32, Batch 83/83, Validation Error 8.114583 %\n",
      "Epoch 32, Batch 83/83, Test Error of Best 7.947917 %\n",
      "Epoch 33, Batch 83/83, Validation Error 8.104167 %\n",
      "Epoch 33, Batch 83/83, Test Error of Best 7.927083 %\n",
      "Epoch 34, Batch 83/83, Validation Error 8.104167 %\n",
      "Epoch 35, Batch 83/83, Validation Error 8.083333 %\n",
      "Epoch 35, Batch 83/83, Test Error of Best 7.885417 %\n",
      "Epoch 36, Batch 83/83, Validation Error 8.052083 %\n",
      "Epoch 36, Batch 83/83, Test Error of Best 7.854167 %\n",
      "Epoch 37, Batch 83/83, Validation Error 8.062500 %\n",
      "Epoch 38, Batch 83/83, Validation Error 8.062500 %\n",
      "Epoch 39, Batch 83/83, Validation Error 8.062500 %\n",
      "Epoch 40, Batch 83/83, Validation Error 8.020833 %\n",
      "Epoch 40, Batch 83/83, Test Error of Best 7.812500 %\n",
      "Epoch 41, Batch 83/83, Validation Error 8.000000 %\n",
      "Epoch 41, Batch 83/83, Test Error of Best 7.833333 %\n",
      "Epoch 42, Batch 83/83, Validation Error 7.968750 %\n",
      "Epoch 42, Batch 83/83, Test Error of Best 7.791667 %\n",
      "Epoch 43, Batch 83/83, Validation Error 7.927083 %\n",
      "Epoch 43, Batch 83/83, Test Error of Best 7.760417 %\n",
      "Epoch 44, Batch 83/83, Validation Error 7.937500 %\n",
      "Epoch 45, Batch 83/83, Validation Error 7.916667 %\n",
      "Epoch 45, Batch 83/83, Test Error of Best 7.718750 %\n",
      "Epoch 46, Batch 83/83, Validation Error 7.906250 %\n",
      "Epoch 46, Batch 83/83, Test Error of Best 7.750000 %\n",
      "Epoch 47, Batch 83/83, Validation Error 7.875000 %\n",
      "Epoch 47, Batch 83/83, Test Error of Best 7.739583 %\n",
      "Epoch 48, Batch 83/83, Validation Error 7.854167 %\n",
      "Epoch 48, Batch 83/83, Test Error of Best 7.677083 %\n",
      "Epoch 49, Batch 83/83, Validation Error 7.843750 %\n",
      "Epoch 49, Batch 83/83, Test Error of Best 7.656250 %\n",
      "Epoch 50, Batch 83/83, Validation Error 7.843750 %\n",
      "Epoch 50, Batch 83/83, Test Error of Best 7.635417 %\n",
      "Epoch 51, Batch 83/83, Validation Error 7.822917 %\n",
      "Epoch 51, Batch 83/83, Test Error of Best 7.625000 %\n",
      "Epoch 52, Batch 83/83, Validation Error 7.802083 %\n",
      "Epoch 52, Batch 83/83, Test Error of Best 7.625000 %\n",
      "Epoch 53, Batch 83/83, Validation Error 7.781250 %\n",
      "Epoch 53, Batch 83/83, Test Error of Best 7.614583 %\n",
      "Epoch 54, Batch 83/83, Validation Error 7.770833 %\n",
      "Epoch 54, Batch 83/83, Test Error of Best 7.614583 %\n",
      "Epoch 55, Batch 83/83, Validation Error 7.760417 %\n",
      "Epoch 55, Batch 83/83, Test Error of Best 7.614583 %\n",
      "Epoch 56, Batch 83/83, Validation Error 7.750000 %\n",
      "Epoch 56, Batch 83/83, Test Error of Best 7.614583 %\n",
      "Epoch 57, Batch 83/83, Validation Error 7.760417 %\n",
      "Epoch 58, Batch 83/83, Validation Error 7.750000 %\n",
      "Epoch 59, Batch 83/83, Validation Error 7.739583 %\n",
      "Epoch 59, Batch 83/83, Test Error of Best 7.625000 %\n",
      "Epoch 60, Batch 83/83, Validation Error 7.718750 %\n",
      "Epoch 60, Batch 83/83, Test Error of Best 7.604167 %\n",
      "Epoch 61, Batch 83/83, Validation Error 7.708333 %\n",
      "Epoch 61, Batch 83/83, Test Error of Best 7.604167 %\n",
      "Epoch 62, Batch 83/83, Validation Error 7.697917 %\n",
      "Epoch 62, Batch 83/83, Test Error of Best 7.593750 %\n",
      "Epoch 63, Batch 83/83, Validation Error 7.687500 %\n",
      "Epoch 63, Batch 83/83, Test Error of Best 7.593750 %\n",
      "Epoch 64, Batch 83/83, Validation Error 7.677083 %\n",
      "Epoch 64, Batch 83/83, Test Error of Best 7.593750 %\n",
      "Epoch 65, Batch 83/83, Validation Error 7.666667 %\n",
      "Epoch 65, Batch 83/83, Test Error of Best 7.593750 %\n",
      "Epoch 66, Batch 83/83, Validation Error 7.687500 %\n",
      "Epoch 67, Batch 83/83, Validation Error 7.677083 %\n",
      "Epoch 68, Batch 83/83, Validation Error 7.656250 %\n",
      "Epoch 68, Batch 83/83, Test Error of Best 7.541667 %\n",
      "Epoch 69, Batch 83/83, Validation Error 7.645833 %\n",
      "Epoch 69, Batch 83/83, Test Error of Best 7.531250 %\n",
      "Epoch 70, Batch 83/83, Validation Error 7.635417 %\n",
      "Epoch 70, Batch 83/83, Test Error of Best 7.520833 %\n",
      "Epoch 71, Batch 83/83, Validation Error 7.614583 %\n",
      "Epoch 71, Batch 83/83, Test Error of Best 7.520833 %\n",
      "Epoch 72, Batch 83/83, Validation Error 7.604167 %\n",
      "Epoch 72, Batch 83/83, Test Error of Best 7.500000 %\n",
      "Epoch 73, Batch 83/83, Validation Error 7.604167 %\n",
      "Epoch 74, Batch 83/83, Validation Error 7.604167 %\n",
      "Epoch 75, Batch 83/83, Validation Error 7.604167 %\n",
      "Epoch 76, Batch 83/83, Validation Error 7.604167 %\n",
      "Epoch 77, Batch 83/83, Validation Error 7.593750 %\n",
      "Epoch 77, Batch 83/83, Test Error of Best 7.479167 %\n",
      "Epoch 78, Batch 83/83, Validation Error 7.583333 %\n",
      "Epoch 78, Batch 83/83, Test Error of Best 7.479167 %\n",
      "Epoch 79, Batch 83/83, Validation Error 7.562500 %\n",
      "Epoch 79, Batch 83/83, Test Error of Best 7.489583 %\n",
      "Epoch 80, Batch 83/83, Validation Error 7.562500 %\n",
      "Epoch 81, Batch 83/83, Validation Error 7.552083 %\n",
      "Epoch 81, Batch 83/83, Test Error of Best 7.510417 %\n",
      "Epoch 82, Batch 83/83, Validation Error 7.552083 %\n",
      "Epoch 83, Batch 83/83, Validation Error 7.541667 %\n",
      "Epoch 83, Batch 83/83, Test Error of Best 7.500000 %\n",
      "Epoch 84, Batch 83/83, Validation Error 7.531250 %\n",
      "Epoch 84, Batch 83/83, Test Error of Best 7.489583 %\n",
      "Epoch 85, Batch 83/83, Validation Error 7.541667 %\n",
      "Epoch 86, Batch 83/83, Validation Error 7.541667 %\n",
      "Optimization complete with best validation score of 7.531250 %, best test performance 7.489583 % \n",
      "The code run for 86 epochs, with 8.694509 epochs/sec\n"
     ]
    }
   ],
   "source": [
    "sgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 46147/50000 (92.294%)\n",
      "Test:\n",
      "Accuracy: 9225/10000 (92.25%)\n",
      "CPU times: user 4.19 s, sys: 1.28 s, total: 5.47 s\n",
      "Wall time: 5.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print \"Train:\"\n",
    "evaluate_mnist(50000, test=False)\n",
    "print \"Test:\"\n",
    "evaluate_mnist(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
